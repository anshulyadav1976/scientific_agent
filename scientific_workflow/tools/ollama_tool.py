"""Tool for interacting with an Ollama API endpoint."""
import logging
import httpx # Use httpx for async requests
from typing import Optional

from pydantic import BaseModel, Field, HttpUrl

from portia.tool import Tool, ToolRunContext, ToolHardError, ToolSoftError

logger = logging.getLogger(__name__)

# --- OllamaTool --- #

class OllamaInputSchema(BaseModel):
    prompt: str = Field(..., description="The complete prompt to send to the Ollama model.")
    ollama_url: str = Field(..., description="The base URL of the Ollama instance (e.g., http://localhost:11434).")
    model_name: str = Field(default="llama3", description="The name of the model to use (e.g., llama3, mistral).") # Default to llama3
    # Add optional parameters like temperature, context window later if needed
    # Example: temperature: Optional[float] = Field(default=None, description="Temperature for sampling.")

    # Basic URL validation
    # @validator('ollama_url')
    # def check_ollama_url(cls, v):
    #     # More robust validation could ping the endpoint, but this is basic
    #     if not v.startswith('http://') and not v.startswith('https://'):
    #         raise ValueError("Ollama URL must start with http:// or https://")
    #     return v

class OllamaTool(Tool[str]): # Initially returns the full string response
    """Connects to a specified Ollama endpoint and runs a prompt with a given model."""
    id: str = "ollama_tool"
    name: str = "Ollama Interaction Tool"
    description: str = (
        "Sends a prompt to a specified Ollama API endpoint (e.g., http://localhost:11434) "
        "using a specified model (e.g., llama3) and returns the generated text response."
    )
    args_schema: type[BaseModel] = OllamaInputSchema
    output_schema: tuple[str, str] = (
        "string",
        "The full text response generated by the Ollama model."
    )

    # Mark run as async
    async def run(self, ctx: ToolRunContext, prompt: str, ollama_url: str, model_name: str = "llama3") -> str:
        """Sends the prompt to the Ollama API and returns the response."""
        logger.info(f"[{self.id}] Sending prompt to Ollama model '{model_name}' at {ollama_url}")

        # Construct the API endpoint (common one is /api/generate)
        # Adjust if using /api/chat which has a different request/response structure
        api_endpoint = f"{ollama_url.rstrip('/')}/api/generate"

        payload = {
            "model": model_name,
            "prompt": prompt,
            "stream": False # Important: Start with non-streaming for simplicity
            # Add other Ollama parameters here if needed (temperature, system prompt, etc.)
        }
        logger.debug(f"Ollama payload: {payload}")

        try:
            async with httpx.AsyncClient(timeout=300.0) as client: # Long timeout for generation
                response = await client.post(api_endpoint, json=payload)
                response.raise_for_status() # Check for HTTP errors
                data = response.json()

                # Assuming /api/generate with stream=False returns {'response': 'full_text', ...}
                if "response" in data:
                    logger.info(f"[{self.id}] Received successful response from Ollama.")
                    return data["response"]
                else:
                    logger.error(f"[{self.id}] Ollama response missing 'response' key: {data}")
                    raise ToolHardError("Received unexpected response format from Ollama.")

        except httpx.HTTPStatusError as e:
            error_body = e.response.text
            logger.error(f"[{self.id}] Ollama API request failed: {e.response.status_code} - {error_body}")
            raise ToolHardError(f"Ollama API request failed ({e.response.status_code}): {error_body}")
        except httpx.RequestError as e:
            logger.error(f"[{self.id}] Cannot connect to Ollama endpoint {api_endpoint}: {e}")
            raise ToolHardError(f"Cannot connect to Ollama endpoint: {e}")
        except Exception as e:
            logger.error(f"[{self.id}] Unexpected error interacting with Ollama: {e}", exc_info=True)
            raise ToolHardError(f"An unexpected error occurred interacting with Ollama: {e}")
