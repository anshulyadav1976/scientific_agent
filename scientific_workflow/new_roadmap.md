# Scientific Workflow Agent - Roadmap (Ollama, Two-Stage Clarification)

This roadmap outlines the development for an agent using a user-specified Ollama model. It features a mandatory clarification step where Ollama first generates questions, the user answers, and then Ollama performs the full research task uninterrupted.

## Milestone 1: Ollama Connection & Question Generation

*(Goal: Connect to Ollama, run a prompt to generate clarifying questions, and display those questions.)*

-   [ ] **Cleanup**: Remove/comment out old tools (`DataIngestionTool`, `AnalysisTool`) and simplify `tools/__init__.py`. Remove old multi-step plan from `ResearchAgent`.
-   [ ] **`OllamaTool` (Initial) (`scientific_workflow/tools/ollama_tool.py`)**:
    -   [ ] Define Input Schema: `prompt` (string), `ollama_url` (string).
    -   [ ] Implement `run` (async): Sends prompt to Ollama via `httpx`, returns the *full text* response. Handles basic connection errors.
-   [ ] **Register `OllamaTool` (`tools/__init__.py`)**.
-   [ ] **Agent Logic - Stage 1 (`agents/research_agent.py`)**:
    -   [ ] Modify `start_analysis`:
        -   Accepts `user_prompt`, `ollama_url`, optional `file_path`.
        -   *Placeholder:* If file exists, read basic text (proper extraction later).
        -   Define `QUESTION_GEN_PROMPT_TEMPLATE`: Instructs Ollama to analyze input and *only* output a list of clarifying questions needed for the main task.
        -   Format the template with user prompt/text.
        -   Create a Portia plan: **Step 1:** Run `OllamaTool` with the question-gen prompt.
        -   Use `portia.create_plan_run()` and return `run_id`.
    -   [ ] Modify `resume_run`:
        -   Executes the plan (the single `OllamaTool` step).
        *   **After** the step completes, get the result (Ollama's questions).
        *   Manually create a `Clarification` object containing these questions.
        *   Save the clarification to `plan_run.outputs.clarifications`.
        *   Manually set `plan_run.state = PlanRunState.NEED_CLARIFICATION`.
        *   Save the updated `plan_run` using `portia.storage.save_plan_run()`.
-   [ ] **Backend (`main.py`)**:
    -   [ ] Update `/upload` to accept `ollama_url` and call new `start_analysis`.
    -   [ ] Update `/resume` to call `agent.resume_run`.
    -   [ ] Update `/status` to:
        -   Call `agent.get_run_status`.
        -   If state is `NEED_CLARIFICATION`, extract the *first* clarification's prompt (the questions) and return it in the response (e.g., `{"clarification": {"id": ..., "prompt": ...}}`).
        -   Return basic status otherwise.
-   [ ] **Frontend (`templates/index.html`, `static/js/main.js`)**:
    -   [ ] Add Ollama URL input.
    -   [ ] Modify `checkStatus`: If clarification data exists, display it using `displayClarification` and stop polling.
    -   [ ] Comment out/disable clarification form submission for now.
-   [ ] **Testing**: Provide prompt (+/- file). Verify the run pauses, and the UI displays the questions generated by Ollama.

## Milestone 2: Clarification Resolution & Stage 2 Execution

*(Goal: Allow the user to answer questions and trigger the main research phase.)*

-   [ ] **Agent Logic - Clarification Handling & Stage 2 (`agents/research_agent.py`)**:
    -   [ ] Implement `resolve_clarification`:
        -   Retrieves the `PlanRun`.
        -   Stores the user's `response` (answers) in `plan_run.outputs` (e.g., key `user_answers`).
        -   Sets state to `READY_TO_RESUME`.
        -   Saves the updated `plan_run`.
    -   [ ] Enhance `resume_run`:
        -   If state is `READY_TO_RESUME` (meaning clarification was just resolved):
            *   Retrieve stored user answers.
            *   Define `RESEARCH_PROMPT_TEMPLATE`: Instructs Ollama to perform the full task using original prompt, file text, *and user answers*, returning the final report.
            *   Format template.
            *   Create a *new, temporary* Portia plan: **Step 1:** Run `OllamaTool` with the research prompt.
            *   Execute this temporary plan *synchronously* within the `resume_run` method (or use `portia.run()` for simplicity if `OllamaTool` is the only step).
            *   Get the final report from the result.
            *   Update the *original* `plan_run`: set `final_output` to the report, set state to `COMPLETE`.
            *   Save the original `plan_run`.
        -   Else (if state is `NOT_STARTED`): proceed with Stage 1 logic as before.
-   [ ] **Backend (`main.py`)**:
    -   [ ] Re-enable `/clarify` endpoint: Calls `agent.resolve_clarification`, returns status.
    -   [ ] Ensure `/resume` calls the enhanced `agent.resume_run`.
    -   [ ] Ensure `/status` correctly returns `final_output` when `COMPLETE`.
-   [ ] **Frontend (`static/js/main.js`)**:
    -   [ ] Re-enable clarification form submission: Sends request to `/clarify`, then calls `/resume` and restarts polling.
    -   [ ] Ensure `checkStatus` displays the `final_output` correctly when the run completes after Stage 2.
-   [ ] **Testing**: Complete the flow: get questions, answer them via UI, verify the second Ollama call runs, and the final report is displayed.

## Milestone 3: Robust File Handling & Tool Use (Search)

*(Goal: Add proper text extraction for various file types and integrate LLM-triggered web search within Stage 2.)*

-   [ ] **Text Extraction (`main.py` or `utils.py`)**: Implement robust text extraction for PDF, DOCX, etc. Update `/upload` to use it.
-   [ ] **Search Tool**: Implement/reuse and register `SearchTool` (e.g., Tavily).
-   [ ] **LLM-Triggered Search**:
    -   [ ] Define format Ollama must output to request search within the Stage 2 prompt execution.
    -   [ ] Update `OllamaTool`: Intercept search requests during Stage 2 execution, call `SearchTool` directly (requires giving `OllamaTool` access to registry or search tool instance), feed results back to Ollama.
    -   [ ] Update `RESEARCH_PROMPT_TEMPLATE`: Instruct Ollama how to request searches.
-   [ ] **Testing**: Upload various files. Use prompts requiring web search. Verify search is triggered and results incorporated into the final report.

## Milestone 4: Streaming & UI Refinements

*(Goal: Improve responsiveness by streaming Ollama's output during Stage 2 and refine the UI.)*

-   [ ] **Streaming**: Implement streaming from Ollama -> `OllamaTool` -> Backend (WebSocket?) -> Frontend during the Stage 2 research execution.
-   [ ] **Thinking Panel**: Re-evaluate. Could show: Stage 1 Questions, User Answers, Stage 2 Prompt, any Tool Calls during Stage 2. Requires `OllamaTool` to log/return this metadata. Update `/status` and `displayThinkingProcess`.
-   [ ] **UI Polish**: Improve chat, input, status, error handling.

## Milestone 5: Final Testing & Iteration

*(Goal: Ensure robustness and refine prompts.)*

-   [ ] **Prompt Engineering**: Refine `QUESTION_GEN_PROMPT_TEMPLATE` and `RESEARCH_PROMPT_TEMPLATE`.
-   [ ] **Error Handling**: Improve error handling throughout.
-   [ ] **Comprehensive Testing**.
